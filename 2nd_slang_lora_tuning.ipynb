{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1fNlFbavlSGxh6AtAWsmylIydpYTw5k85",
      "authorship_tag": "ABX9TyNhMcvJxQgZJqZ2MOeBd/jH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeoyeonPark1223/Gemma_FineTuning/blob/main/2nd_slang_lora_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "3qqyhgpCtyNQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctL1zdHdsYrt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"KAGGLE_USERNAME\"] = 'trispark'\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get('trispark')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U keras-nlp\n",
        "!pip install -q -U \"keras>=3\""
      ],
      "metadata": {
        "id": "dt3CXSAbs141"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"KERAS_BACKEND\"]= 'jax'\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\""
      ],
      "metadata": {
        "id": "9zaLJT2zs-QV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import keras_nlp"
      ],
      "metadata": {
        "id": "tUs85tbXtK86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "M4tSkNHwttFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset"
      ],
      "metadata": {
        "id": "Pam_-8HGt1Ki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "slang_dataset = pd.read_csv(\"/content/drive/MyDrive/MLB_Kaggle_Gemma/all_slang_only_words.csv\")"
      ],
      "metadata": {
        "id": "s6a9QAhotNEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "slang_data = []\n",
        "\n",
        "for index, row in slang_dataset.iterrows():\n",
        "    # Instruction prompts the user to input the context\n",
        "    instruction = (\n",
        "        \"Given the context below, create a new Gen Z slang term. \",\n",
        "        \"The slang should be catchy, easy to use, and relevant to modern youth culture. \",\n",
        "        \"Make sure it's something that would feel natural in casual conversation:\\n\\n\",\n",
        "        \"Context: \" + row['Context'],\n",
        "        \"Make sure that you should provide slang, description, and example as given.\"\n",
        "    )\n",
        "\n",
        "    # Response provides the description and example for the slang\n",
        "    response = (\n",
        "        \"Slang: {slang}\\n\\n\"\n",
        "        \"Description: {description}\\n\\n\"\n",
        "        \"Example: {example}\".format(\n",
        "            slang=row['Slang'],\n",
        "            description=row['Description'],\n",
        "            example=row['Example']\n",
        "        )\n",
        "    )\n",
        "\n",
        "    template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
        "    slang_data.append(template.format(instruction=instruction, response=response))"
      ],
      "metadata": {
        "id": "5JcoXg2swktN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model + LoRA fine-tuning"
      ],
      "metadata": {
        "id": "gb_vw4oPt9Ua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_2b_en\")"
      ],
      "metadata": {
        "id": "RT2cpwtJtq8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_lm.backbone.enable_lora(rank=8)"
      ],
      "metadata": {
        "id": "s6aBz8nOuMz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_lm.summary()"
      ],
      "metadata": {
        "id": "A6ASwoUvvOj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Limit the input sequence length to 256 (to control memory usage)\n",
        "gemma_lm.preprocessor.sequence_length = 256\n",
        "\n",
        "# Use AdamW (optimizer for transformer models)\n",
        "optimizer = keras.optimizers.AdamW(\n",
        "    learning_rate = 5e-5,\n",
        "    weight_decay = 0.01,\n",
        ")\n",
        "\n",
        "# Exclude layernorm and bias terms from decay\n",
        "optimizer.exclude_from_weight_decay(var_names=['bias', 'scale'])\n",
        "\n",
        "gemma_lm.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=optimizer,\n",
        "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "gemma_lm.fit(slang_data, epochs=10, batch_size=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qD_hzyMtvQHz",
        "outputId": "728975bc-f3fe-4964-e82a-2413dc840173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1779/1779\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1476s\u001b[0m 807ms/step - loss: 0.5051 - sparse_categorical_accuracy: 0.7887\n",
            "Epoch 2/10\n",
            "\u001b[1m1779/1779\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1437s\u001b[0m 794ms/step - loss: 0.2686 - sparse_categorical_accuracy: 0.8726\n",
            "Epoch 3/10\n",
            "\u001b[1m1654/1779\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m1:39\u001b[0m 794ms/step - loss: 0.2570 - sparse_categorical_accuracy: 0.8765"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference (which is soooo bad)"
      ],
      "metadata": {
        "id": "8Mfw0urCF_7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tag = \"Given the context below, create a new Gen Z slang term. The slang should be catchy, easy to use, and relevant to modern youth culture. Make sure it's something that would feel natural in casual conversation:\\n\\n\"\n",
        "\n",
        "prompt = template.format(\n",
        "    instruction = tag + \"casual conversation.\",\n",
        "    response=\"\",\n",
        ")\n",
        "sampler = keras_nlp.samplers.TopKSampler(k=20, seed=2)\n",
        "gemma_lm.compile(sampler=sampler)\n",
        "print(gemma_lm.generate(prompt, max_length=256))"
      ],
      "metadata": {
        "id": "n8sQplhJwZcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DIOerfU6FjKi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}