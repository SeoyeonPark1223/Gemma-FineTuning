{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeoyeonPark1223/Gemma_FineTuning/blob/main/2nd_slang_lora_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "3qqyhgpCtyNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwW3ZjNBXEqw",
        "outputId": "d8b7ef2e-448d-44e0-db5c-6c5734fa4066"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ctL1zdHdsYrt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"KAGGLE_USERNAME\"] = 'trispark'\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get('trispark')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U keras-nlp\n",
        "!pip install -q -U \"keras>=3\""
      ],
      "metadata": {
        "id": "dt3CXSAbs141"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"KERAS_BACKEND\"]= 'jax'\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\""
      ],
      "metadata": {
        "id": "9zaLJT2zs-QV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import keras_nlp"
      ],
      "metadata": {
        "id": "tUs85tbXtK86"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "M4tSkNHwttFX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset"
      ],
      "metadata": {
        "id": "Pam_-8HGt1Ki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "slang_dataset = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/all_slang_only_words.csv\")"
      ],
      "metadata": {
        "id": "s6a9QAhotNEp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "slang_data = []\n",
        "\n",
        "for index, row in slang_dataset.iterrows():\n",
        "    # Instruction prompts the user to input the context\n",
        "    instruction = (\n",
        "        \"Given the context below, create a new Gen Z slang term. \",\n",
        "        \"The slang should be catchy, easy to use, and relevant to modern youth culture. \",\n",
        "        \"Make sure it's something that would feel natural in casual conversation:\\n\\n\",\n",
        "        \"Context: \" + row['Context'],\n",
        "        \"Make sure that you should provide slang, description, and example as given.\"\n",
        "    )\n",
        "\n",
        "    # Response provides the description and example for the slang\n",
        "    response = (\n",
        "        \"Slang: {slang}\\n\\n\"\n",
        "        \"Description: {description}\\n\\n\"\n",
        "        \"Example: {example}\".format(\n",
        "            slang=row['Slang'],\n",
        "            description=row['Description'],\n",
        "            example=row['Example']\n",
        "        )\n",
        "    )\n",
        "\n",
        "    template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
        "    slang_data.append(template.format(instruction=instruction, response=response))"
      ],
      "metadata": {
        "id": "5JcoXg2swktN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model + LoRA fine-tuning"
      ],
      "metadata": {
        "id": "gb_vw4oPt9Ua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_2b_en\")"
      ],
      "metadata": {
        "id": "RT2cpwtJtq8L"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_lm.backbone.enable_lora(rank=8)"
      ],
      "metadata": {
        "id": "s6aBz8nOuMz2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_lm.summary()"
      ],
      "metadata": {
        "id": "A6ASwoUvvOj-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "ad9975c4-8698-4514-8060-0aa84968afa1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n",
              "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
              "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,620,199,168\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
              "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,620,199,168</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
              "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,620,199,168\u001b[0m (9.76 GB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,620,199,168</span> (9.76 GB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,857,280\u001b[0m (22.34 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,857,280</span> (22.34 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Limit the input sequence length to 256 (to control memory usage)\n",
        "gemma_lm.preprocessor.sequence_length = 256\n",
        "\n",
        "# Use AdamW (optimizer for transformer models)\n",
        "optimizer = keras.optimizers.AdamW(\n",
        "    learning_rate = 5e-5,\n",
        "    weight_decay = 0.01,\n",
        ")\n",
        "\n",
        "# Exclude layernorm and bias terms from decay\n",
        "optimizer.exclude_from_weight_decay(var_names=['bias', 'scale'])\n",
        "\n",
        "gemma_lm.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=optimizer,\n",
        "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "gemma_lm.fit(slang_data, epochs=5, batch_size=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qD_hzyMtvQHz",
        "outputId": "a8f3c50d-021d-4df9-9ccf-ce5dd8f49d8a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1779/1779\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1523s\u001b[0m 833ms/step - loss: 0.5060 - sparse_categorical_accuracy: 0.7883\n",
            "Epoch 2/5\n",
            "\u001b[1m1779/1779\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1523s\u001b[0m 843ms/step - loss: 0.2676 - sparse_categorical_accuracy: 0.8725\n",
            "Epoch 3/5\n",
            "\u001b[1m1779/1779\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1463s\u001b[0m 822ms/step - loss: 0.2530 - sparse_categorical_accuracy: 0.8775\n",
            "Epoch 4/5\n",
            "\u001b[1m1779/1779\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1463s\u001b[0m 822ms/step - loss: 0.2390 - sparse_categorical_accuracy: 0.8822\n",
            "Epoch 5/5\n",
            "\u001b[1m1779/1779\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1453s\u001b[0m 817ms/step - loss: 0.2235 - sparse_categorical_accuracy: 0.8888\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a7ce9b0ef50>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference (which is soooo bad)"
      ],
      "metadata": {
        "id": "8Mfw0urCF_7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tag = (\n",
        "    \"Given the context below, create a new slang term. \"\n",
        "    \"The slang should be catchy, easy to use, and relevant to modern youth culture. \"\n",
        "    \"Make sure it's something that would feel natural in casual conversation:\\n\\n\"\n",
        ")\n",
        "\n",
        "context = \"You're hanging out with friends at school just chatting in recess\"\n",
        "\n",
        "condition = \"You should suggest new slang and its definition, also give some examples for clarification. Example should be long and also precise.\"\n",
        "\n",
        "prompt = template.format(\n",
        "    instruction = tag + context + condition,\n",
        "    response=\"\",\n",
        ")\n",
        "\n",
        "sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n",
        "gemma_lm.compile(sampler=sampler)\n",
        "\n",
        "output = gemma_lm.generate(prompt, max_length=512)\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "id": "n8sQplhJwZcj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30df4242-d100-4b57-a9d8-371b7c12d840"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instruction:\n",
            "Given the context below, create a new slang term. The slang should be catchy, easy to use, and relevant to modern youth culture. Make sure it's something that would feel natural in casual conversation:\n",
            "\n",
            "You're hanging out with friends at school just chatting in recessYou should suggest new slang and its definition, also give some examples for clarification. Example should be long and also precise.\n",
            "\n",
            "Response:\n",
            "Slang: ZH\n",
            "\n",
            "Context: Hang out\n",
            "\n",
            "Description: Hang out.\n",
            "\n",
            "Usage: I’ll ZH with you.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tag = (\n",
        "    \"Given the context below, create a new slang term. \"\n",
        "    \"The slang should be catchy, easy to use, and relevant to modern youth culture. \"\n",
        "    \"Make sure it's something that would feel natural in casual conversation:\\n\\n\"\n",
        ")\n",
        "\n",
        "context = \"You're hanging out with friends at a restaurant drinking wine.\"\n",
        "\n",
        "condition = \"You should suggest a new slang and its definition, also give one example for clarification. Example should be long and also precise.\"\n",
        "\n",
        "prompt = template.format(\n",
        "    instruction = tag + context + condition,\n",
        "    response=\"\",\n",
        ")\n",
        "\n",
        "sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n",
        "gemma_lm.compile(sampler=sampler)\n",
        "\n",
        "output = gemma_lm.generate(prompt, max_length=512)\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvLZW7o41FBE",
        "outputId": "12af8f05-1afb-4543-af87-f0aeb684c05e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instruction:\n",
            "Given the context below, create a new slang term. The slang should be catchy, easy to use, and relevant to modern youth culture. Make sure it's something that would feel natural in casual conversation:\n",
            "\n",
            "You're hanging out with friends at a restaurant drinking wine.You should suggest a new slang and its definition, also give one example for clarification. Example should be long and also precise.\n",
            "\n",
            "Response:\n",
            "Slang: YWIAW\n",
            "\n",
            "Context: You’re welcome, it’s wine and appetizers.\n",
            "\n",
            "Example: Let’s order some food—YWIAW.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tag = (\n",
        "    \"Given the context below, create a new slang term. \"\n",
        "    \"The slang should be catchy, easy to use, and relevant to modern youth culture. \"\n",
        "    \"Make sure it's something that would feel natural in casual conversation:\\n\\n\"\n",
        ")\n",
        "\n",
        "context = \"You're at your office working on your project with your teammates\"\n",
        "\n",
        "condition = \"You should suggest a new slang and its definition, also give one example for clarification. Example should be long and also precise.\"\n",
        "\n",
        "prompt = template.format(\n",
        "    instruction = tag + context + condition,\n",
        "    response=\"\",\n",
        ")\n",
        "\n",
        "sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n",
        "gemma_lm.compile(sampler=sampler)\n",
        "\n",
        "output = gemma_lm.generate(prompt, max_length=512)\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GH3LfKTF1u4w",
        "outputId": "10e6427e-2522-4af8-8008-51e2220b8e75"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instruction:\n",
            "Given the context below, create a new slang term. The slang should be catchy, easy to use, and relevant to modern youth culture. Make sure it's something that would feel natural in casual conversation:\n",
            "\n",
            "You're at your office working on your project with your teammatesYou should suggest a new slang and its definition, also give one example for clarification. Example should be long and also precise.\n",
            "\n",
            "Response:\n",
            "Slang: ZYN\n",
            "\n",
            "Context: Zero your network\n",
            "\n",
            "Description: Refers to disconnecting from the internet to prevent any data leaks\n",
            "\n",
            "Use: Zyn for added security\n",
            "\n",
            "#\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-vI4hxYP4jJi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}